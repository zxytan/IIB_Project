{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09/10/20\n",
    "Post-vacation contact with supervisor - added to Slack and given paper on Bayesian Machine learning in metamaterial design to read (https://onlinelibrary-wiley-com.ezp.lib.cam.ac.uk/doi/full/10.1002/adma.201904845#adma201904845-fig-0002)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/10/20\n",
    "Testing GPyOpt functionality with their manual examples and functions from https://en.wikipedia.org/wiki/Test_functions_for_optimization. 1D and 2D straightforward with visuals. Tried with higher dimensional problems (n>=10) - sphere function was fine, but less optimal when the function is rough and needs more iterations (local minima)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12/10/20\n",
    "Formalise testing - see gpyopt_test.py\n",
    "Added timing - 15 iterations for 2D Booth function and 10D sphere function both took around 5s to run, 40 iterations for 10D Rastrigin function took 13s with f(x) error of 0.8. Not bad!\n",
    "Made github repo for project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13/10/20\n",
    "Call with supervisor to outline next steps:\n",
    "* Further testing with GPyOpt\n",
    "    * evaluation time, no. of evaluations to optimum\n",
    "    * compare different acquisition functions\n",
    "    * test with non-linear functions (introduce discontinuities)\n",
    "    * sparse Gaussian processes to lower compute time\n",
    "* Consider multi-fidelity Gaussian processes - data from multiple sources\n",
    "* Look into deep learning methods for optimisation - fast.ai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25/10/20\n",
    "Acquisition function $$u(x)$$ guides finding new samples in objective function. Balance exploration (samples from unexplored/high uncertainty areas) and exploitation (samples around current optimum, drill down to true (local) optimum). Formally, take sample where $$\\mathbf{x}=\\underset{\\mathbf{x}}{\\arg \\max} u(x|\\mathcal{D}_{1:t-1})$$ where $$\\mathcal{D}_{1:t-1}$$ is data up to this point. Most common acquisition functions are Maximum Probability of Improvement (MPI), Expected Improvement (EI) and Upper Confidence Bound (UCB).\n",
    "\n",
    "Testing of GPyOpt - evaluation time and accuracy when varying noise, maximum iterations, acquisition function, dimension, function type (convex?). Results given in gpyopt_test_results.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
